{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3df27ab-9cd4-4d97-afae-cd060e10c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b1bcb2-c1ed-4017-9137-7dbcf7e75e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96365205-d2e8-47fa-bdbd-dc7bbd05b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"MyAppFromNotebook\").master(\"local[*]\").config(\"spark.executor.memory\", \"2g\").config(\"spark.driver.memory\", \"1g\").getOrCreate()\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ScalaToPySpark\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277b4dc0-6ead-4cba-91e3-3ab37216465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://A33.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ScalaToPySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=ScalaToPySpark>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc41912b-b427-4422-8b34-1aa46e8987c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23464 Jps\n",
      "17420 SparkSubmit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to get the \n",
    "import subprocess\n",
    "\n",
    "result = subprocess.check_output(\"jps\", shell=True)\n",
    "print(result.decode())\n",
    "#It lists all the Java processes running on your system along with their process ID (PID) and main class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8704d8bc-5264-4d08-afc0-f175830fdd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://A33.lan:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.uiWebUrl) #to access spark ui directly click on the link to get the Spark GUI page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a3be86-62dc-4f68-8cec-694ad6a3bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sc.parallelize(range(1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2e405b-8374-4234-88d5-6b9cf17e0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = data1.map(lambda x: x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45e7370b-1db0-43de-b529-c6da2362c6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 25,\n",
       " 36,\n",
       " 49,\n",
       " 64,\n",
       " 81,\n",
       " 100,\n",
       " 121,\n",
       " 144,\n",
       " 169,\n",
       " 196,\n",
       " 225,\n",
       " 256,\n",
       " 289,\n",
       " 324,\n",
       " 361,\n",
       " 400]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ee6c23-f9db-4483-a40f-acfb813b8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"file:///C://Users/aksha/Pyspark/testData1.log\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e608a65-eb60-4101-8ae1-47c4c03156fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatfile = file.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff1328e-5d36-438c-a96f-5ce06812d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapfile = flatfile.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42742764-6256-4c27-8a3e-e5b096559b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "redfile = mapfile.reduceByKey(lambda a,b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ab2a11-8388-4448-b4e2-dbf7396c1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata = redfile.sortBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92e6e2c-0a79-47ea-918e-d2122bb48732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be: 2\n",
      "demo: 3\n",
      "every: 1\n",
      "hadoop: 1\n",
      "have: 1\n",
      "holiday: 1\n",
      "is: 2\n",
      "not: 2\n",
      "on: 2\n",
      "sunday: 4\n",
      "there: 2\n",
      "today: 2\n",
      "we: 1\n",
      "will: 2\n"
     ]
    }
   ],
   "source": [
    "for word, count in sortdata.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa034c6-0d5f-4929-89c5-6f7fd452c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount = file.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c71e80a0-b467-41bc-a22b-bc7ac02dac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be: 2\n",
      "demo: 3\n",
      "every: 1\n",
      "hadoop: 1\n",
      "have: 1\n",
      "holiday: 1\n",
      "is: 2\n",
      "not: 2\n",
      "on: 2\n",
      "sunday: 4\n",
      "there: 2\n",
      "today: 2\n",
      "we: 1\n",
      "will: 2\n"
     ]
    }
   ],
   "source": [
    "for word, count in wordcount.collect(): \n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcac65d4-846f-4376-84a6-6ef52c74f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"file:///C://Users/aksha/Pyspark/Input.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1c4a8b0-6a21-43e9-bcfa-88806f683aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "errorFile = file.filter(lambda x: \"Spark\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec6a933-f781-4b0d-9c35-906f28b7d3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d08b9b9-448b-46c8-866b-3b0e2878c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkdata = errorFile.filter(lambda x: x.startswith(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53e2fc58-3315-4357-acc8-7d5ddbf49926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5436de59-317a-4d2f-b030-e2c5d6486616",
   "metadata": {},
   "outputs": [],
   "source": [
    "endspark = errorFile.filter(lambda x: x.endswith(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5732b1f8-8477-4f20-a447-28c558de7b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52e1691-12c5-43c5-ad60-cf29472669b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[31] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorFile.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d98655-9c5e-4455-a18c-02c93baf46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23676a15-d281-45f1-9243-11d7a00e2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkspark = errorFile.filter(lambda x: x.startswith(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9771fcef-6a63-444f-81c4-3aaba7c0ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a298fd6-c708-453d-8024-7fcdcd5a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "endspark = errorFile.filter(lambda x: x.endswith(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9155cba-3e22-4ba0-a545-6dbc345a08e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a664b748-720d-4277-abb9-059f5cc69920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errorFile.persist(StorageLevel.MEMORY_AND_DISK) # Spark doesn’t allow changing the storage level of an RDD once it's been persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f5c363-2eb6-4087-a85b-b350b3abdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISK_ONLY', 'DISK_ONLY_2', 'DISK_ONLY_3', 'MEMORY_AND_DISK', 'MEMORY_AND_DISK_2', 'MEMORY_AND_DISK_DESER', 'MEMORY_ONLY', 'MEMORY_ONLY_2', 'NONE', 'OFF_HEAP', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(StorageLevel)) #Can use if the storage level which exists in this list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fe90d95-94cf-4dee-87d0-746074018025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[31] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorFile.unpersist(True) # If we want to change the StorageLevel to other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "317c4f27-f92b-4294-9d49-fd5bf1a5c577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[31] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorFile.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb9d76-43ea-4239-8b2a-3c3a46866c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Python 3.10)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
