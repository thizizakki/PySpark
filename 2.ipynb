{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8521a5e-11bf-4115-b767-7c1b043a2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f09ec6-f569-478a-a55d-d974f8f73f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65d3941-729c-4dc6-a2da-1e00437e4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133e37d7-6c72-42a6-9d97-06d636198828",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ScalaToPySpark\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3129a05f-cc62-4822-8f99-4ec21b8eaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"file:///C://Users/aksha/Pyspark/Input.log\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f94a8c-186f-48f0-b0d5-d8a717b5d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9205e4-b1d6-475b-b996-d4889fe0171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c61181f-156d-410d-8f0b-44bb73ff08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(1,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e7c962-6abe-48b2-b860-6f61d8e1a7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb71f5a-b210-4fe9-a50f-ce1892edd6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20644daf-9044-44dc-8ea3-580727dc2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "repartData = data.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44878a45-cf4e-4d6a-b0b0-56e65d4b76ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c5297b-9fb0-480f-87f7-cc570ae5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartData.saveAsTextFile(\"file:///C:\\\\Users\\\\aksha\\\\Pyspark\\\\output\")  # Escaped backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e943ff4e-442e-4b00-8c88-c02b8c69bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: [6, 7, 13, 14, 15]\n",
      "Partition 1: []\n",
      "Partition 2: [1, 2, 3, 4, 5, 26, 27]\n",
      "Partition 3: []\n",
      "Partition 4: [18, 19, 20, 36, 37]\n",
      "Partition 5: [11, 12, 21, 22, 31, 32, 33, 34, 35, 38, 39, 40]\n",
      "Partition 6: [8, 9, 10, 23, 24, 25, 28, 29, 30]\n",
      "Partition 7: [16, 17]\n"
     ]
    }
   ],
   "source": [
    "for idx, part in repartData.mapPartitionsWithIndex(lambda idx, it: [(idx, list(it))]).collect():\n",
    "    print(f\"Partition {idx}: {part}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692557fd-7d02-4a27-8b44-9ae4084fee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coadata = data.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27dd5dd9-5d51-4895-967f-2befd2ac5239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coadata.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6236ae75-5f91-4b41-bf90-02db222bb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coadata.saveAsTextFile(\"file:///C:\\\\Users\\\\aksha\\\\Pyspark\\\\output\")  # Escaped backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88b8c9d-2cb2-4227-95e2-1213c5ebb86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Partition 1: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Partition 2: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Partition 3: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n"
     ]
    }
   ],
   "source": [
    "for idx, part in coadata.mapPartitionsWithIndex(lambda idx, it: [(idx, list(it))]).collect():\n",
    "    print(f\"Partition {idx}: {part}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9376503d-380d-470b-8b3d-bc1dbf863f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = sc.parallelize(range(1,41),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb14a4b0-5383-4437-ba50-ba6c30b08475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45be0ab4-912c-4af2-9784-25184972a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFile = sc.textFile(\"file:///C://Users/aksha/Pyspark/Input.log\",5)  # Base RDD Level Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30ae6350-ae39-45d1-9663-b442123f3590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFile.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba5a629a-1d85-47f2-b8f1-2e7a84d14fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e1d98d-9e56-402a-97f4-3b83d78fdae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glomdata = data.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f00f05d-01fd-4c43-bd63-b5da4903b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [2],\n",
       " [3],\n",
       " [4, 5],\n",
       " [6],\n",
       " [7],\n",
       " [8],\n",
       " [9, 10],\n",
       " [11],\n",
       " [12],\n",
       " [13],\n",
       " [14, 15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19, 20]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glomdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6939cac7-f0d9-4784-83a7-36341e120dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "glomfile = newFile.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6357b08a-50e7-40b4-acb0-00384a7da2f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Learning Spark is fun and very powerful',\n",
       "  'PySpark lets you process big data in memory',\n",
       "  'DataFrames are optimized for large datasets',\n",
       "  'Spark transformations are lazy by default',\n",
       "  'Actions like collect trigger the execution',\n",
       "  'Map and reduce are the classic RDD tools',\n",
       "  'Broadcast variables reduce data movement',\n",
       "  'Shuffles are expensive in distributed systems',\n",
       "  'You can cache RDDs to reuse in later steps',\n",
       "  'Spark SQL integrates seamlessly with Hive',\n",
       "  'Filter and select are DataFrame operations',\n",
       "  'Window functions enable advanced analytics',\n",
       "  'Partitioning helps with data distribution',\n",
       "  'FlatMap can produce multiple outputs per line',\n",
       "  'GroupBy followed by agg is common pattern',\n",
       "  'Joins can be expensive if not handled wisely',\n",
       "  'Cluster managers like YARN help Spark scale',\n",
       "  'Checkpointing helps in fault-tolerant systems',\n",
       "  'RDDs are the low-level API in Spark engine',\n",
       "  'SparkSession is the entry point in PySpark',\n",
       "  'You can read CSV JSON Parquet Avro formats'],\n",
       " ['Structured Streaming enables real-time apps',\n",
       "  'Schema inference can save you manual effort',\n",
       "  'Spark is written in Scala but supports Python',\n",
       "  'The DAG is built before execution starts',\n",
       "  'Spark works on local or cluster deployments',\n",
       "  'SparkContext gives access to low-level API',\n",
       "  'Spark is fast due to in-memory computation',\n",
       "  'Operations in Spark are immutable in nature',\n",
       "  'Resilient Distributed Datasets = RDDs',\n",
       "  'Data locality improves performance in Spark',\n",
       "  'You can register DataFrames as SQL tables',\n",
       "  'Catalyst optimizer plans efficient execution',\n",
       "  'Tungsten engine handles code generation',\n",
       "  'MLlib provides machine learning algorithms',\n",
       "  'RDD transformations are narrow or wide',\n",
       "  'Repartition increases the number of partitions',\n",
       "  'Coalesce reduces the number of partitions',\n",
       "  'PartitionBy is used during file writing',\n",
       "  'Use cache or persist to keep data in memory',\n",
       "  'Memory management is important for tuning',\n",
       "  'Executors run the actual Spark tasks'],\n",
       " ['Spark jobs have multiple stages and tasks',\n",
       "  'Wide dependencies require data shuffling',\n",
       "  'HDFS and S3 are common storage backends',\n",
       "  'Read operations are lazy in Spark',\n",
       "  'Actions like count, show trigger execution',\n",
       "  'Custom UDFs allow for flexible transformations',\n",
       "  'Python UDFs are slower than native functions',\n",
       "  'Use Spark UI to debug jobs and performance',\n",
       "  'Accumulator variables help in debugging logic',\n",
       "  'You can broadcast small lookup tables',\n",
       "  'Use explain() to view the logical plan',\n",
       "  'Spark supports JDBC for reading databases',\n",
       "  'collect() brings entire dataset to driver',\n",
       "  'Take action returns limited number of rows',\n",
       "  'RDDs are not schema-aware like DataFrames',\n",
       "  'Schema evolution works better in Parquet',\n",
       "  'Avoid using collect() on large datasets',\n",
       "  'Spark supports columnar formats for speed',\n",
       "  'Use sample() to draw random subset of data',\n",
       "  'sampleBy() helps with stratified sampling',\n",
       "  'DataFrames are row-based internally in Spark'],\n",
       " ['Use orderBy or sort to arrange rows',\n",
       "  'Spark streaming uses micro-batching',\n",
       "  'Structured streaming has lower latency',\n",
       "  'Kafka is used as input source in streaming',\n",
       "  'Watermarking handles late-arriving data',\n",
       "  'Join conditions should be optimized properly',\n",
       "  'Zip allows pairing elements from two RDDs',\n",
       "  'You can use mapPartitions for custom logic',\n",
       "  'Spark has transformations and actions clearly',\n",
       "  'Always tune number of partitions for scaling',\n",
       "  'Partition pruning reduces scan time in queries',\n",
       "  'Avoid skewed joins by salting the keys',\n",
       "  'SessionState tracks current Spark session info',\n",
       "  'RDD lineage graph helps in debugging',\n",
       "  'Debugging in distributed systems is tricky',\n",
       "  'DataFrame API is faster than RDD API',\n",
       "  'Explode is used to flatten array-type fields',\n",
       "  'Pivot tables can be created using groupBy',\n",
       "  'Python APIs wrap underlying Java methods',\n",
       "  'Parquet is preferred for columnar storage',\n",
       "  'Use write.partitionBy to save partitioned data'],\n",
       " ['Column objects allow for chaining transformations',\n",
       "  'Always filter early to reduce data transfer',\n",
       "  'Use distinct() to get unique values from DF',\n",
       "  'SQLContext is deprecated in Spark 3.x',\n",
       "  'Broadcast joins are great for small dimension tables',\n",
       "  'countByValue returns count per distinct value',\n",
       "  'Use reduce to aggregate values in RDDs',\n",
       "  'Always prefer DataFrames when possible',\n",
       "  'Avoid collecting large RDDs to the driver',\n",
       "  'Use zipWithIndex for row numbering',\n",
       "  'Each executor can run multiple tasks in parallel',\n",
       "  'Driver node is the coordinator in Spark apps',\n",
       "  'Use randomSplit() for ML train/test split',\n",
       "  'Use printSchema() to inspect data structure',\n",
       "  'Parquet is splittable and supports predicate pushdown',\n",
       "  'DataFrames are untyped in PySpark but typed in Scala',\n",
       "  'collectAsMap returns key-value dict from pair RDD',\n",
       "  'Show truncates long columns by default',\n",
       "  'Chaining multiple transformations is efficient',\n",
       "  'Spark is resilient and fault-tolerant']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glomfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a4bc471-4a2d-41b2-887c-e6abacb458b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sc.parallelize([\"KA\",\"MH\",\"TN\",\"TS\",\"DL\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69912cb1-0306-4d62-a705-c354ae8edc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cef267c6-64c3-4c97-ad03-461a4a4cba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KA'], ['MH'], ['TN'], ['TS', 'DL']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d16e825-662e-4f3d-b796-fef6b6323a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = sc.parallelize([\"Ben\",\"Mum\",\"Chn\",\"Hyd\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfb80bf3-a173-4e8d-817e-844932513854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5e87382-16cb-4425-9124-90a330632579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ben'], ['Mum'], ['Chn'], ['Hyd']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14e1ee9b-2a04-41b4-8bb8-f529e3898371",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata = states.zip(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6492cba-0d84-43ad-bcd7-a447e9826492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, partition in zipdata.collect():\n",
    " #   print(f\"Partition {idx}: {partition}\") Since the no elements in both files are not matching this snippet gonna throw error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ced541ce-7872-4a22-bc9b-fc9c147bf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "states1 = sc.parallelize([\"KA\",\"MH\",\"TN\",\"TS\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cbe2f61-7e04-4f43-95e0-9ed30b242689",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities1 = sc.parallelize([\"Ben\",\"Mum\",\"Chn\",\"Hyd\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78dc3d53-404a-4f59-8a27-56fd8be2348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0bfd769-8f17-4bd8-b64b-dde6639a5849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f38b820-8005-4a1d-9335-3db0d7c771c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata1 = states1.zip(cities1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a15213d-29a0-4c97-a1ea-126e1e987e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition KA: Ben\n",
      "partition MH: Mum\n",
      "partition TN: Chn\n",
      "partition TS: Hyd\n"
     ]
    }
   ],
   "source": [
    "for idx, partition in zipdata1.collect():\n",
    "    print(f\"partition {idx}: {partition}\") # here all the values in both data are matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5eae2b1-88ad-4aa9-b189-7313238a255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "states2 = sc.parallelize([\"KA\",\"MH\",\"TN\",\"TS\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc0726fa-9f65-4b97-be87-a54c20eab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = sc.parallelize([\"Ben\",\"Mum\",\"Chn\",\"Hyd\"],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6092e20f-2b63-4f58-b9be-e532ee830243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipdata2 = states2.zip(cities2) # Can only zip with RDD which has the same number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44a74f99-d5d8-42b1-ade7-6bbf46af50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipIndexData = zipdata1.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "621d3ff5-56bc-481d-ae3a-953e4cebfb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('KA', 'Ben'), 0)\n",
      "(('MH', 'Mum'), 1)\n",
      "(('TN', 'Chn'), 2)\n",
      "(('TS', 'Hyd'), 3)\n"
     ]
    }
   ],
   "source": [
    "for i in zipIndexData.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "513861f8-f20d-4acb-9789-aadd1252371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([90,20,10,20,10,90,90,90,90,90,90,10,30,45,55,87,98,34,54,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59778519-ce31-4f56-af1c-762cc91cdc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0201fbc8-2209-4481-b695-f6d763a9502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distdata = data.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cab4370-abd2-4fe8-932b-7907dd23e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90ac0fc4-0c12-41ba-b182-bee18e837493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 34, 20, 54, 55, 87, 90, 10, 45, 30]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b05618e3-194f-4ccd-9c24-2ffc7d9b743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanew = sc.parallelize([\"Spark\",\"Scala\",\"Python\",\"Spark\",\"spark\",\"Scala\",\"Spark\",\"Python\",\"Spark\",\"SCala\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "794196f8-d376-47a6-923b-f4fd1586ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "distdata = datanew.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f41f9a89-9ea6-4dda-92bd-27d498b41278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7799afab-b976-47d7-a45f-0a1f23d1e1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'SCala', 'Spark', 'Scala', 'Python']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f5b1f18-e1ad-40e2-a8a0-25467d4570d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = datanew.map(lambda a: (a, 1)).groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b903120-364d-403e-a692-62cc50f5e234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: [1]\n",
      "SCala: [1]\n",
      "Spark: [1, 1, 1, 1]\n",
      "Scala: [1, 1]\n",
      "Python: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "for key, values in grouped.collect():\n",
    "    print(f\"{key}: {list(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b0c5ca4-1417-433e-aa07-e55dc9f7be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: 1\n",
      "SCala: 1\n",
      "Spark: 4\n",
      "Scala: 2\n",
      "Python: 2\n"
     ]
    }
   ],
   "source": [
    "for key, count in grouped.collect():\n",
    "    print(f\"{key}: {len(count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73fd197f-48fb-4198-b8a9-e0370700a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using functions within transformation\n",
    "def gkfun1(p):\n",
    "    return p * p * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf97d9a0-121d-4f0f-8e36-cd2c002afcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([3, 6, 1, 3, 9, 10, 14, 11, 12, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff776420-a50e-4a4f-91ca-badcca9ca614",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = data.map(gkfun1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4634e415-c485-4cd0-b859-6d1ee49f00df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 216, 1, 27, 729, 1000, 2744, 1331, 1728, 1000]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c793c56-2826-4c47-8f8d-1db1b81940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gkfun2(x):\n",
    "    return \"Hello\" + \" \" + x.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7aabc967-eb8b-495c-8e10-3dcf07d37e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([\"Spark\",\"sCala\",\"python\",\"hadOOp\",\"RubyOnRails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd2e09f4-b470-4aeb-a7d1-881d0f8674a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = data.map(gkfun2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1689494b-6d06-4c34-a95b-e0c4f9ca4769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello SPARK',\n",
       " 'Hello SCALA',\n",
       " 'Hello PYTHON',\n",
       " 'Hello HADOOP',\n",
       " 'Hello RUBYONRAILS']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cd54b32-837e-4ec3-8aa5-d747288bc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata = sc.parallelize([1,2,3,4,5,6,7,8],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa7a388b-6c59-45d0-9f57-df3d3064e825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7, 8]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f65ae4c7-dbf7-4c99-b882-40c6d76ff844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(x):\n",
    "    sum1 = 1\n",
    "    return (sum1 + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30470060-0681-46c3-a3d4-089b05c7bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newInputdata = inputdata.map(sum) # funtion for map transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e237e91-8f12-42a6-9f0d-a23c64432d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newInputdata.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bad2a7fc-1694-44d2-92a3-42222df31d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_part(iter):\n",
    "    total = 1\n",
    "    for x in iter:\n",
    "        total += x\n",
    "    return [total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71206f90-1c31-48db-b1ba-b01c5d4e1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.parallelize([1,3,5,7,9,11,13,15,17,19],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0048a39e-aef6-422d-92ae-aaba57eec30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = file.mapPartitions(sum_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "975e236b-f1d1-4959-8987-1235c078f5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 76]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40eb1348-7a97-4018-97a1-9b678def0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the key-value RDD\n",
    "data = sc.parallelize([(\"Spark\", 40),(\"Scala\", 60),(\"Hadoop\", 20),(\"C++\", 40),(\"C\", 50),(\"Java\", 30),(\"Python\", 50),(\"RubyOnRails\", 40),(\"VC++\", 70),\n",
    "    (\"Cobol\", 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f788925f-48c9-4e88-8ef7-5849592c9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract keys only\n",
    "keydata = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "801cc674-d2db-49ed-ae7c-1458c05fe116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "Scala\n",
      "Hadoop\n",
      "C++\n",
      "C\n",
      "Java\n",
      "Python\n",
      "RubyOnRails\n",
      "VC++\n",
      "Cobol\n"
     ]
    }
   ],
   "source": [
    "for key in keydata.collect():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b9f68b3-0fac-464e-9222-b193114ae8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuedata = data.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd7dc134-d668-482f-a5de-cb7dab62f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "60\n",
      "20\n",
      "40\n",
      "50\n",
      "30\n",
      "50\n",
      "40\n",
      "70\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for value in valuedata.collect():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3123ff6-72c4-4815-9525-b083baf633cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([34, 12, 10, 41, 23, 56, 77, 72, 10, 98, 87, 57, 15, 32, 67,89, 99, 20, 30, 40, 50, 60, 80, 90, 97, 81, 38, 39, 26])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ac0163b-3541-4545-bd0e-c129e7ced27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e459ea79-f2ed-40b0-8974-ef48d0abfcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 12, 10, 41]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(4) # First 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4ce2d4a-ae00-43d6-adab-589a0c5b6ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99, 98, 97, 90]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.top(4) # Top 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9bd22e79-518d-4a53-8e51-5a8bd1599fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 12, 15]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.takeOrdered(4) # Below 4 elements that means descending order that too least 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "28bc65c7-cd8b-4ab3-bb06-715bb50fb93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 12, 15, 20]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.distinct().takeOrdered(4) # Below 4 elements that means descending order that too least 4 distinct elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d736c25-c4f0-40d0-982b-d653768ea5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12a7979c-f213-423f-9eef-08b0546e70b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "007f4669-aac0-46f8-8acd-757af624018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.75862068965517"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b1cfe9f9-c238-48b7-8c81-954eab6c88d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(28.906096687095438)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5672228-1ece-4db5-a6e1-b49d1ca9c735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "17d25acc-5801-40ea-a8f7-3a5e422fc346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5fd05093-aff4-4b3e-b3c4-eff25b3d1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3 = sc.parallelize([\"Spark\", \"Scala\", \"Java\", \"C++\", \"Python\", \"Cobot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b4cc55d1-a80d-4bdb-9253-c7aa18b313e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data4 = sc.parallelize([\"JavaScript\", \"Hadoop\", \"Scala\", \"Java\", \"Informatica\", \"DataStage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d965d8f4-322c-42b7-a4c4-2b1aba28a86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('J', ('a', 'a')), ('J', ('a', 'a')), ('S', ('p', 'c')), ('S', ('c', 'c'))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data3.join(data4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a64fb-74a1-4d80-bd65-88e20d570ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302efbdf-a9cd-4381-86aa-42d3812baad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Python 3.10)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
