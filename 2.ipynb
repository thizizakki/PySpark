{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8521a5e-11bf-4115-b767-7c1b043a2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f09ec6-f569-478a-a55d-d974f8f73f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65d3941-729c-4dc6-a2da-1e00437e4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133e37d7-6c72-42a6-9d97-06d636198828",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ScalaToPySpark\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3129a05f-cc62-4822-8f99-4ec21b8eaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"file:///C://Users/aksha/Pyspark/Input.log\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f94a8c-186f-48f0-b0d5-d8a717b5d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9205e4-b1d6-475b-b996-d4889fe0171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c61181f-156d-410d-8f0b-44bb73ff08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(1,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e7c962-6abe-48b2-b860-6f61d8e1a7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb71f5a-b210-4fe9-a50f-ce1892edd6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20644daf-9044-44dc-8ea3-580727dc2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "repartData = data.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44878a45-cf4e-4d6a-b0b0-56e65d4b76ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c5297b-9fb0-480f-87f7-cc570ae5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartData.saveAsTextFile(\"file:///C:\\\\Users\\\\aksha\\\\Pyspark\\\\output\")  # Escaped backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a811c1f4-7e42-462e-8044-8db44d6b0ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [6, 7, 13, 14, 15]),\n",
       " (1, []),\n",
       " (2, [1, 2, 3, 4, 5, 26, 27]),\n",
       " (3, []),\n",
       " (4, [18, 19, 20, 36, 37]),\n",
       " (5, [11, 12, 21, 22, 31, 32, 33, 34, 35, 38, 39, 40]),\n",
       " (6, [8, 9, 10, 23, 24, 25, 28, 29, 30]),\n",
       " (7, [16, 17])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inspect_partitions(rdd):   #Data within the partitioned RDDs\n",
    "    return (rdd.mapPartitionsWithIndex(lambda idx, it: [(idx, list(it))]).collect())\n",
    "inspect_partitions(repartData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692557fd-7d02-4a27-8b44-9ae4084fee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coadata = data.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27dd5dd9-5d51-4895-967f-2befd2ac5239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coadata.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6236ae75-5f91-4b41-bf90-02db222bb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coadata.saveAsTextFile(\"file:///C:\\\\Users\\\\aksha\\\\Pyspark\\\\output\")  # Escaped backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88b8c9d-2cb2-4227-95e2-1213c5ebb86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]),\n",
       " (1,\n",
       "  [21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inspect_partitions(rdd):   #Data within the partitioned RDDs\n",
    "    return (rdd.mapPartitionsWithIndex(lambda idx, it: [(idx, list(it))]).collect())\n",
    "inspect_partitions(coadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9376503d-380d-470b-8b3d-bc1dbf863f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = sc.parallelize(range(1,41),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb14a4b0-5383-4437-ba50-ba6c30b08475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45be0ab4-912c-4af2-9784-25184972a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFile = sc.textFile(\"file:///C://Users/aksha/Pyspark/Input.log\",5)  # Base RDD Level Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30ae6350-ae39-45d1-9663-b442123f3590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFile.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba5a629a-1d85-47f2-b8f1-2e7a84d14fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e1d98d-9e56-402a-97f4-3b83d78fdae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glomdata = data.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f00f05d-01fd-4c43-bd63-b5da4903b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [2],\n",
       " [3],\n",
       " [4, 5],\n",
       " [6],\n",
       " [7],\n",
       " [8],\n",
       " [9, 10],\n",
       " [11],\n",
       " [12],\n",
       " [13],\n",
       " [14, 15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19, 20]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glomdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6939cac7-f0d9-4784-83a7-36341e120dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "glomfile = newFile.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6357b08a-50e7-40b4-acb0-00384a7da2f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Learning Spark is fun and very powerful',\n",
       "  'PySpark lets you process big data in memory',\n",
       "  'DataFrames are optimized for large datasets',\n",
       "  'Spark transformations are lazy by default',\n",
       "  'Actions like collect trigger the execution',\n",
       "  'Map and reduce are the classic RDD tools',\n",
       "  'Broadcast variables reduce data movement',\n",
       "  'Shuffles are expensive in distributed systems',\n",
       "  'You can cache RDDs to reuse in later steps',\n",
       "  'Spark SQL integrates seamlessly with Hive',\n",
       "  'Filter and select are DataFrame operations',\n",
       "  'Window functions enable advanced analytics',\n",
       "  'Partitioning helps with data distribution',\n",
       "  'FlatMap can produce multiple outputs per line',\n",
       "  'GroupBy followed by agg is common pattern',\n",
       "  'Joins can be expensive if not handled wisely',\n",
       "  'Cluster managers like YARN help Spark scale',\n",
       "  'Checkpointing helps in fault-tolerant systems',\n",
       "  'RDDs are the low-level API in Spark engine',\n",
       "  'SparkSession is the entry point in PySpark',\n",
       "  'You can read CSV JSON Parquet Avro formats'],\n",
       " ['Structured Streaming enables real-time apps',\n",
       "  'Schema inference can save you manual effort',\n",
       "  'Spark is written in Scala but supports Python',\n",
       "  'The DAG is built before execution starts',\n",
       "  'Spark works on local or cluster deployments',\n",
       "  'SparkContext gives access to low-level API',\n",
       "  'Spark is fast due to in-memory computation',\n",
       "  'Operations in Spark are immutable in nature',\n",
       "  'Resilient Distributed Datasets = RDDs',\n",
       "  'Data locality improves performance in Spark',\n",
       "  'You can register DataFrames as SQL tables',\n",
       "  'Catalyst optimizer plans efficient execution',\n",
       "  'Tungsten engine handles code generation',\n",
       "  'MLlib provides machine learning algorithms',\n",
       "  'RDD transformations are narrow or wide',\n",
       "  'Repartition increases the number of partitions',\n",
       "  'Coalesce reduces the number of partitions',\n",
       "  'PartitionBy is used during file writing',\n",
       "  'Use cache or persist to keep data in memory',\n",
       "  'Memory management is important for tuning',\n",
       "  'Executors run the actual Spark tasks'],\n",
       " ['Spark jobs have multiple stages and tasks',\n",
       "  'Wide dependencies require data shuffling',\n",
       "  'HDFS and S3 are common storage backends',\n",
       "  'Read operations are lazy in Spark',\n",
       "  'Actions like count, show trigger execution',\n",
       "  'Custom UDFs allow for flexible transformations',\n",
       "  'Python UDFs are slower than native functions',\n",
       "  'Use Spark UI to debug jobs and performance',\n",
       "  'Accumulator variables help in debugging logic',\n",
       "  'You can broadcast small lookup tables',\n",
       "  'Use explain() to view the logical plan',\n",
       "  'Spark supports JDBC for reading databases',\n",
       "  'collect() brings entire dataset to driver',\n",
       "  'Take action returns limited number of rows',\n",
       "  'RDDs are not schema-aware like DataFrames',\n",
       "  'Schema evolution works better in Parquet',\n",
       "  'Avoid using collect() on large datasets',\n",
       "  'Spark supports columnar formats for speed',\n",
       "  'Use sample() to draw random subset of data',\n",
       "  'sampleBy() helps with stratified sampling',\n",
       "  'DataFrames are row-based internally in Spark'],\n",
       " ['Use orderBy or sort to arrange rows',\n",
       "  'Spark streaming uses micro-batching',\n",
       "  'Structured streaming has lower latency',\n",
       "  'Kafka is used as input source in streaming',\n",
       "  'Watermarking handles late-arriving data',\n",
       "  'Join conditions should be optimized properly',\n",
       "  'Zip allows pairing elements from two RDDs',\n",
       "  'You can use mapPartitions for custom logic',\n",
       "  'Spark has transformations and actions clearly',\n",
       "  'Always tune number of partitions for scaling',\n",
       "  'Partition pruning reduces scan time in queries',\n",
       "  'Avoid skewed joins by salting the keys',\n",
       "  'SessionState tracks current Spark session info',\n",
       "  'RDD lineage graph helps in debugging',\n",
       "  'Debugging in distributed systems is tricky',\n",
       "  'DataFrame API is faster than RDD API',\n",
       "  'Explode is used to flatten array-type fields',\n",
       "  'Pivot tables can be created using groupBy',\n",
       "  'Python APIs wrap underlying Java methods',\n",
       "  'Parquet is preferred for columnar storage',\n",
       "  'Use write.partitionBy to save partitioned data'],\n",
       " ['Column objects allow for chaining transformations',\n",
       "  'Always filter early to reduce data transfer',\n",
       "  'Use distinct() to get unique values from DF',\n",
       "  'SQLContext is deprecated in Spark 3.x',\n",
       "  'Broadcast joins are great for small dimension tables',\n",
       "  'countByValue returns count per distinct value',\n",
       "  'Use reduce to aggregate values in RDDs',\n",
       "  'Always prefer DataFrames when possible',\n",
       "  'Avoid collecting large RDDs to the driver',\n",
       "  'Use zipWithIndex for row numbering',\n",
       "  'Each executor can run multiple tasks in parallel',\n",
       "  'Driver node is the coordinator in Spark apps',\n",
       "  'Use randomSplit() for ML train/test split',\n",
       "  'Use printSchema() to inspect data structure',\n",
       "  'Parquet is splittable and supports predicate pushdown',\n",
       "  'DataFrames are untyped in PySpark but typed in Scala',\n",
       "  'collectAsMap returns key-value dict from pair RDD',\n",
       "  'Show truncates long columns by default',\n",
       "  'Chaining multiple transformations is efficient',\n",
       "  'Spark is resilient and fault-tolerant']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glomfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a4bc471-4a2d-41b2-887c-e6abacb458b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sc.parallelize([\"KA\",\"MH\",\"TN\",\"TS\",\"DL\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69912cb1-0306-4d62-a705-c354ae8edc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cef267c6-64c3-4c97-ad03-461a4a4cba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KA'], ['MH'], ['TN'], ['TS', 'DL']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d16e825-662e-4f3d-b796-fef6b6323a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = sc.parallelize([\"Ben\",\"Mum\",\"Chn\",\"Hyd\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfb80bf3-a173-4e8d-817e-844932513854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5e87382-16cb-4425-9124-90a330632579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ben'], ['Mum'], ['Chn'], ['Hyd']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14e1ee9b-2a04-41b4-8bb8-f529e3898371",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata = states.zip(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6492cba-0d84-43ad-bcd7-a447e9826492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, partition in zipdata.collect():\n",
    " #   print(f\"Partition {idx}: {partition}\") Since the no elements in both files are not matching this snippet gonna throw error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ced541ce-7872-4a22-bc9b-fc9c147bf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "states1 = sc.parallelize([\"KA\",\"MH\",\"TN\",\"TS\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cbe2f61-7e04-4f43-95e0-9ed30b242689",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities1 = sc.parallelize([\"Ben\",\"Mum\",\"Chn\",\"Hyd\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78dc3d53-404a-4f59-8a27-56fd8be2348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0bfd769-8f17-4bd8-b64b-dde6639a5849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f38b820-8005-4a1d-9335-3db0d7c771c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata1 = states1.zip(cities1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a15213d-29a0-4c97-a1ea-126e1e987e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition KA: Ben\n",
      "partition MH: Mum\n",
      "partition TN: Chn\n",
      "partition TS: Hyd\n"
     ]
    }
   ],
   "source": [
    "for idx, partition in zipdata1.collect():\n",
    "    print(f\"partition {idx}: {partition}\") # here all the values in both data are matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5eae2b1-88ad-4aa9-b189-7313238a255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "states2 = sc.parallelize([\"KA\",\"MH\",\"TN\",\"TS\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc0726fa-9f65-4b97-be87-a54c20eab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = sc.parallelize([\"Ben\",\"Mum\",\"Chn\",\"Hyd\"],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6092e20f-2b63-4f58-b9be-e532ee830243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipdata2 = states2.zip(cities2) # Can only zip with RDD which has the same number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44a74f99-d5d8-42b1-ade7-6bbf46af50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipIndexData = zipdata1.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "621d3ff5-56bc-481d-ae3a-953e4cebfb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('KA', 'Ben'), 0)\n",
      "(('MH', 'Mum'), 1)\n",
      "(('TN', 'Chn'), 2)\n",
      "(('TS', 'Hyd'), 3)\n"
     ]
    }
   ],
   "source": [
    "for i in zipIndexData.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513861f8-f20d-4acb-9789-aadd1252371e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Python 3.10)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
