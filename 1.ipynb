{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b37a92-2068-436d-aace-f9f3ac9ee29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58866a5-6e8f-4f4a-8e44-64d3ec614393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51817780-bccf-4fb5-9de4-420e6aaaa060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:\\\\spark-3.5.6-bin-hadoop3\"\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"C:\\\\spark-3.5.6-bin-hadoop3\\\\bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48798b40-03d9-4c8f-959d-e61f29a7b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ScalaToPySpark\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b479b6-f9ed-48c8-b5e6-210d4e60a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"file:///C:/Users/aksha/Pyspark/Input.log\") \n",
    "#If the specified path is wrong still it doesnt throw error. Only at the action level it \n",
    "# throws error because spark follows bottom top approach that means only when action is triggered then only it throws error if the file doesnt exist \n",
    "# in the specified path'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b14d1e-b604-45e8-a6aa-2c4280e1eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapfile = file.map(lambda x: len(x)) #Equivalent Scala code: mapfile = file.map(x => x.lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa72d03-d880-4f12-bdbe-dd71d0cdb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapfile = file.map(lambda x: (x, len(x))) #Equivalent Scala code: mapfile = file.map(x => (x,x.lenth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a1d58f-3e92-4dc0-9af1-d4fd9bd83c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mapfile.collect())  #print(mapfile.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037fca13-11f5-49c2-88ae-0562138373d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Spark is fun and very powerful,39\n",
      "PySpark lets you process big data in memory,43\n",
      "DataFrames are optimized for large datasets,43\n",
      "Spark transformations are lazy by default,41\n",
      "Actions like collect trigger the execution,42\n",
      "Map and reduce are the classic RDD tools,40\n",
      "Broadcast variables reduce data movement,40\n",
      "Shuffles are expensive in distributed systems,45\n",
      "You can cache RDDs to reuse in later steps,42\n",
      "Spark SQL integrates seamlessly with Hive,41\n",
      "Filter and select are DataFrame operations,42\n",
      "Window functions enable advanced analytics,42\n",
      "Partitioning helps with data distribution,41\n",
      "FlatMap can produce multiple outputs per line,45\n",
      "GroupBy followed by agg is common pattern,41\n",
      "Joins can be expensive if not handled wisely,44\n",
      "Cluster managers like YARN help Spark scale,43\n",
      "Checkpointing helps in fault-tolerant systems,45\n",
      "RDDs are the low-level API in Spark engine,42\n",
      "SparkSession is the entry point in PySpark,42\n",
      "You can read CSV JSON Parquet Avro formats,42\n",
      "Structured Streaming enables real-time apps,43\n",
      "Schema inference can save you manual effort,43\n",
      "Spark is written in Scala but supports Python,45\n",
      "The DAG is built before execution starts,40\n",
      "Spark works on local or cluster deployments,43\n",
      "SparkContext gives access to low-level API,42\n",
      "Spark is fast due to in-memory computation,42\n",
      "Operations in Spark are immutable in nature,43\n",
      "Resilient Distributed Datasets = RDDs,37\n",
      "Data locality improves performance in Spark,43\n",
      "You can register DataFrames as SQL tables,41\n",
      "Catalyst optimizer plans efficient execution,44\n",
      "Tungsten engine handles code generation,39\n",
      "MLlib provides machine learning algorithms,42\n",
      "RDD transformations are narrow or wide,38\n",
      "Repartition increases the number of partitions,46\n",
      "Coalesce reduces the number of partitions,41\n",
      "PartitionBy is used during file writing,39\n",
      "Use cache or persist to keep data in memory,43\n",
      "Memory management is important for tuning,41\n",
      "Executors run the actual Spark tasks,36\n",
      "Spark jobs have multiple stages and tasks,41\n",
      "Wide dependencies require data shuffling,40\n",
      "HDFS and S3 are common storage backends,39\n",
      "Read operations are lazy in Spark,33\n",
      "Actions like count, show trigger execution,42\n",
      "Custom UDFs allow for flexible transformations,46\n",
      "Python UDFs are slower than native functions,44\n",
      "Use Spark UI to debug jobs and performance,42\n",
      "Accumulator variables help in debugging logic,45\n",
      "You can broadcast small lookup tables,37\n",
      "Use explain() to view the logical plan,38\n",
      "Spark supports JDBC for reading databases,41\n",
      "collect() brings entire dataset to driver,41\n",
      "Take action returns limited number of rows,42\n",
      "RDDs are not schema-aware like DataFrames,41\n",
      "Schema evolution works better in Parquet,40\n",
      "Avoid using collect() on large datasets,39\n",
      "Spark supports columnar formats for speed,41\n",
      "Use sample() to draw random subset of data,42\n",
      "sampleBy() helps with stratified sampling,41\n",
      "DataFrames are row-based internally in Spark,44\n",
      "Use orderBy or sort to arrange rows,35\n",
      "Spark streaming uses micro-batching,35\n",
      "Structured streaming has lower latency,38\n",
      "Kafka is used as input source in streaming,42\n",
      "Watermarking handles late-arriving data,39\n",
      "Join conditions should be optimized properly,44\n",
      "Zip allows pairing elements from two RDDs,41\n",
      "You can use mapPartitions for custom logic,42\n",
      "Spark has transformations and actions clearly,45\n",
      "Always tune number of partitions for scaling,44\n",
      "Partition pruning reduces scan time in queries,46\n",
      "Avoid skewed joins by salting the keys,38\n",
      "SessionState tracks current Spark session info,46\n",
      "RDD lineage graph helps in debugging,36\n",
      "Debugging in distributed systems is tricky,42\n",
      "DataFrame API is faster than RDD API,36\n",
      "Explode is used to flatten array-type fields,44\n",
      "Pivot tables can be created using groupBy,41\n",
      "Python APIs wrap underlying Java methods,40\n",
      "Parquet is preferred for columnar storage,41\n",
      "Use write.partitionBy to save partitioned data,46\n",
      "Column objects allow for chaining transformations,49\n",
      "Always filter early to reduce data transfer,43\n",
      "Use distinct() to get unique values from DF,43\n",
      "SQLContext is deprecated in Spark 3.x,37\n",
      "Broadcast joins are great for small dimension tables,52\n",
      "countByValue returns count per distinct value,45\n",
      "Use reduce to aggregate values in RDDs,38\n",
      "Always prefer DataFrames when possible,38\n",
      "Avoid collecting large RDDs to the driver,41\n",
      "Use zipWithIndex for row numbering,34\n",
      "Each executor can run multiple tasks in parallel,48\n",
      "Driver node is the coordinator in Spark apps,44\n",
      "Use randomSplit() for ML train/test split,41\n",
      "Use printSchema() to inspect data structure,43\n",
      "Parquet is splittable and supports predicate pushdown,53\n",
      "DataFrames are untyped in PySpark but typed in Scala,52\n",
      "collectAsMap returns key-value dict from pair RDD,49\n",
      "Show truncates long columns by default,38\n",
      "Chaining multiple transformations is efficient,46\n",
      "Spark is resilient and fault-tolerant,37\n"
     ]
    }
   ],
   "source": [
    "for line, length in mapfile.collect():  #mapfile.collect.foreach(println)\n",
    "    print(f\"{line},{length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717a1f58-ebda-4707-8ad1-34ed0894c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mapfile.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86755b45-8e6a-4460-8933-635591977114",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([\"Spark\",\"Scala\", \"Hadoop\",\"C++\",\"C\",\"Java\",\"Python\",\"RubyOnRails\",\"VC++\",\"Cobol\",\"JavaScript\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfaedf7-2096-43d3-9b5d-9d455fed0ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03f57ab-9cf8-48a1-82e8-e65c37efc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = data.map(lambda x:(x, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8437b2b9-9bc5-4c94-be9f-c3a06c4ebfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark,5\n",
      "Scala,5\n",
      "Hadoop,6\n",
      "C++,3\n",
      "C,1\n",
      "Java,4\n",
      "Python,6\n",
      "RubyOnRails,11\n",
      "VC++,4\n",
      "Cobol,5\n",
      "JavaScript,10\n"
     ]
    }
   ],
   "source": [
    "for line,length in mapdata.collect():\n",
    "    print(f\"{line},{length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f67dc09-3a60-461b-ab48-caf4da956177",
   "metadata": {},
   "outputs": [],
   "source": [
    "filfile = file.filter(lambda x: len(x) > 45)  # Here file refers to the input file which is read at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e5f515-3f42-4d96-aeef-8bee6ecc18f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "154bdbe8-8fa2-4ba8-ab73-2f58b73f2ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition increases the number of partitions,46\n",
      "Custom UDFs allow for flexible transformations,46\n",
      "Partition pruning reduces scan time in queries,46\n",
      "SessionState tracks current Spark session info,46\n",
      "Use write.partitionBy to save partitioned data,46\n",
      "Column objects allow for chaining transformations,49\n",
      "Broadcast joins are great for small dimension tables,52\n",
      "Each executor can run multiple tasks in parallel,48\n",
      "Parquet is splittable and supports predicate pushdown,53\n",
      "DataFrames are untyped in PySpark but typed in Scala,52\n",
      "collectAsMap returns key-value dict from pair RDD,49\n",
      "Chaining multiple transformations is efficient,46\n"
     ]
    }
   ],
   "source": [
    "for line in filfile.collect():\n",
    "    print(f\"{line},{len(line)}\") #Here accessing line which are filter i.e, lines greater than 45 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90218581-fc1a-47b1-9826-6b434edbfbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkfile = file.filter(lambda a:\"transformations \" in a)\n",
    "sparkfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c592ed-1cfb-44a5-afb7-def8861e3ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark transformations are lazy by default\n",
      "RDD transformations are narrow or wide\n",
      "Spark has transformations and actions clearly\n",
      "Chaining multiple transformations is efficient\n"
     ]
    }
   ],
   "source": [
    "for line in sparkfile.collect():\n",
    "    print(f\"{line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23716051-8d8a-463b-84a9-e62698553e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkfile = file.filter(lambda a:\"Spark\" in a and \"RDD\" in a) #for multiple level filters\n",
    "sparkfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0618198b-ea4b-454e-a982-3e9c75954614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDDs are the low-level API in Spark engine\n"
     ]
    }
   ],
   "source": [
    "for line in sparkfile.collect():\n",
    "    print(f\"{line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5a773fa-8413-455e-9eb5-a38fdc027ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkfile.saveAsTextFile(\"file:///C:/Users/aksha/Pyspark/output\") #Currently not getting error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bfd1e9d-bed1-4358-b93e-a238e6572345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkfile.getNumPartitions()  # No of partitions allotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78fdaca7-d009-4c51-8843-c1754bf3f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "readfile = file.filter(lambda a: a.startswith(\"Spark\")) #Lines start with the word \"Spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8574bbbe-7bef-4d9b-857f-3b9600f7a4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d27d1621-6707-40ba-b54a-43636cd8afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark transformations are lazy by default\n",
      "Spark SQL integrates seamlessly with Hive\n",
      "SparkSession is the entry point in PySpark\n",
      "Spark is written in Scala but supports Python\n",
      "Spark works on local or cluster deployments\n",
      "SparkContext gives access to low-level API\n",
      "Spark is fast due to in-memory computation\n",
      "Spark jobs have multiple stages and tasks\n",
      "Spark supports JDBC for reading databases\n",
      "Spark supports columnar formats for speed\n",
      "Spark streaming uses micro-batching\n",
      "Spark has transformations and actions clearly\n",
      "Spark is resilient and fault-tolerant\n"
     ]
    }
   ],
   "source": [
    "for line in readfile.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8313ad2d-ccac-454e-ba95-136a51fa1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "readfile = file.filter(lambda a: a.endswith(\"Spark\")) #Lines ends with the word \"Spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84aeceef-673f-4e72-9293-273fc5aae6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa0aea7-45ff-4581-a5cb-4b2861fb9117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession is the entry point in PySpark\n",
      "Data locality improves performance in Spark\n",
      "Read operations are lazy in Spark\n",
      "DataFrames are row-based internally in Spark\n"
     ]
    }
   ],
   "source": [
    "for line in readfile.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2565181-0335-4f93-8788-554c80b512c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fildata = data.filter(lambda x: len(x) > 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d161a68-bf3b-4123-a885-4abd5da75df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b89622a-996b-4241-8ce1-b86e9b84686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RubyOnRails\n",
      "JavaScript\n"
     ]
    }
   ],
   "source": [
    "for line in fildata.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a446e64-6327-477c-af8c-443f80831d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filda = data.filter(lambda x: \"a\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea9d8771-a98d-4e65-a39d-b0d2d88121e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "Scala\n",
      "Hadoop\n",
      "Java\n",
      "RubyOnRails\n",
      "JavaScript\n"
     ]
    }
   ],
   "source": [
    "for line in filda.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39d7b87-d876-41d1-a627-fe112e28d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = data.map(lambda x: (x, len(x))).filter(lambda x: x[1] > 7)  # in a single line 2 transformations are applied                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5337d515-f9e8-4769-8c1a-7d060ff1a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RubyOnRails,11\n",
      "JavaScript,10\n"
     ]
    }
   ],
   "source": [
    "for line,leng in fill.collect():\n",
    "    print(f\"{line},{leng}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d79013b-d956-4e47-9481-36d87ba4fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark,5\n",
      "Scala,5\n",
      "Hadoop,6\n",
      "C++,3\n",
      "C,1\n",
      "Java,4\n",
      "Python,6\n",
      "RubyOnRails,11\n",
      "VC++,4\n",
      "Cobol,5\n",
      "JavaScript,10\n"
     ]
    }
   ],
   "source": [
    "for line, leng in mapdata.collect():\n",
    "    print(f\"{line},{leng}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d804fc33-00b4-44f8-910e-1317418c14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 =sc.textFile(\"file:///C://Users/aksha/Pyspark/testData.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4336bf5f-58e7-4884-a97f-a6f47712d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatfile = file2.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3094c224-b5b9-4eae-91cf-fd4ff6c02b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f13f920-fcb8-4896-88a3-c0ca8c43987f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19f61f9b-cd80-4b10-ba38-d6855e174ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today\n",
      "is\n",
      "Saturday\n",
      "today\n",
      "we\n",
      "have\n",
      "Hadoop\n",
      "demo\n",
      "demo\n",
      "will\n",
      "not\n",
      "be\n",
      "there\n",
      "on\n",
      "every\n",
      "Saturday\n"
     ]
    }
   ],
   "source": [
    "for word in flatfile.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6640ca47-c42b-4460-a26b-6f4b7829540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatfile = file2.flatMap(lambda x: x.split(\" \")).map(lambda y: (y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84895930-e3a1-4169-ad43-67f142ed9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('today', 1)\n",
      "('is', 1)\n",
      "('Saturday', 1)\n",
      "('today', 1)\n",
      "('we', 1)\n",
      "('have', 1)\n",
      "('Hadoop', 1)\n",
      "('demo', 1)\n",
      "('demo', 1)\n",
      "('will', 1)\n",
      "('not', 1)\n",
      "('be', 1)\n",
      "('there', 1)\n",
      "('on', 1)\n",
      "('every', 1)\n",
      "('Saturday', 1)\n"
     ]
    }
   ],
   "source": [
    "for word in flatfile.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f3740b0-447f-47e9-a7df-e2a7c2160ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = sc.parallelize([(\"Spark\", 40),(\"Scala\", 60),(\"Hadoop\", 20),(\"C++\", 40),(\"C\", 71),(\"Java\", 30),(\"Python\", 40),(\"RubyOnRails\", 79),(\"VC++\", 65),\n",
    "    (\"Cobol\", 10),(\"JavaScript\", 90)]) #PairedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bdb681a-b918-4044-87a2-20bb6f3e11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = datas.map(lambda x: (x[0][::-1],x[1] + 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60664853-c3b9-4a79-b91c-7300aa897434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('krapS', 140)\n",
      "('alacS', 160)\n",
      "('poodaH', 120)\n",
      "('++C', 140)\n",
      "('C', 171)\n",
      "('avaJ', 130)\n",
      "('nohtyP', 140)\n",
      "('sliaRnOybuR', 179)\n",
      "('++CV', 165)\n",
      "('loboC', 110)\n",
      "('tpircSavaJ', 190)\n"
     ]
    }
   ],
   "source": [
    "for word in maps.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4900cd43-a5ff-4a88-bd68-2e3470dd5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpas = datas.map(lambda x: (x,x[1]+100)).filter(lambda x: (x[1] > 130)) # Here it is filtering the data which is having the marks greater than 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53ffad2d-58a2-4410-bd0c-07cc393badad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Spark', 40), 140)\n",
      "(('Scala', 60), 160)\n",
      "(('C++', 40), 140)\n",
      "(('C', 71), 171)\n",
      "(('Python', 40), 140)\n",
      "(('RubyOnRails', 79), 179)\n",
      "(('VC++', 65), 165)\n",
      "(('JavaScript', 90), 190)\n"
     ]
    }
   ],
   "source": [
    "for word in mpas.collect(): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a5cb660-9359-49ee-b8d8-6a27c4c30b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this transformation applies only on values.\n",
    "mapvalData = datas.mapValues(lambda x: x+100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5a5aa58-c630-4f94-a1f9-8fa0838de848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spark', 140)\n",
      "('Scala', 160)\n",
      "('Hadoop', 120)\n",
      "('C++', 140)\n",
      "('C', 171)\n",
      "('Java', 130)\n",
      "('Python', 140)\n",
      "('RubyOnRails', 179)\n",
      "('VC++', 165)\n",
      "('Cobol', 110)\n",
      "('JavaScript', 190)\n"
     ]
    }
   ],
   "source": [
    "for word in mapvalData.collect(): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cfa16e2-64ad-4bbf-8c4d-c2f19fe0c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = sc.parallelize([\"Spark\", \"Scala\", \"Hadoop\", \"C++\", \"C\", \"Java\", \"Python\", \"RubyOnRails\", \"VC++\", \"Cobol\", \"JavaScript\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00b2ef88-d489-40c6-98c2-70248006df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = datas.map(lambda x: (x, len(x))).mapValues(lambda x: x + 10) #Since the data is not in key value paid we have to convert the data to key \n",
    "#value paired. then we can apply mapValues transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90b79234-12f7-4a7b-a11b-8b3d346b188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spark', 15)\n",
      "('Scala', 15)\n",
      "('Hadoop', 16)\n",
      "('C++', 13)\n",
      "('C', 11)\n",
      "('Java', 14)\n",
      "('Python', 16)\n",
      "('RubyOnRails', 21)\n",
      "('VC++', 14)\n",
      "('Cobol', 15)\n",
      "('JavaScript', 20)\n"
     ]
    }
   ],
   "source": [
    "for word in mapdata.collect(): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b7cb971-881c-49b4-9215-5398763a64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile1 = sc.textFile(\"file:///C://Users/aksha/Pyspark/file1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d09d845-84ce-43eb-83ed-3668bb8bb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile2 = sc.textFile(\"file:///C://Users/aksha/Pyspark/file2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "777bf5f6-75f7-44a8-9933-8a211ff5eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = txtfile1.union(txtfile2) # clubs the data from 2 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21f945e6-954d-4e6e-b19c-e82b4365a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6efc8ce1-53c2-4718-aed5-1a373f82130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop is bigdata tool\n",
      "Hadoop is good for analytics\n",
      "Sark is meant for only processing\n",
      "Spark is purely depending on IN_MEMORY processing\n",
      "Hadoop and Spark are going to rule bigdata market\n",
      "thanks Hadoop\n",
      "thanks Bigdata\n",
      "thanks Hadoop\n",
      "Hadoop HDFS is for storage\n",
      "Hadoop MR is for processing\n",
      "Hadoop is having diff distributions support\n",
      "Hadoop is good in market\n",
      "thanks Bigdata\n",
      "thanks Spark\n"
     ]
    }
   ],
   "source": [
    "for word in result.collect(): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8156165-09e6-44ea-bf90-472cdd04d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = txtfile1.intersection(txtfile2) #common data from both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "821255ce-abfc-4da2-8233-301b0ebd7642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d874b36a-5549-4f80-b63b-9d553961894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thanks Hadoop\n",
      "thanks Bigdata\n"
     ]
    }
   ],
   "source": [
    "for word in result.collect(): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7266f785-8d6d-4905-81a6-10b3fb87c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = txtfile1.subtract(txtfile2) #what is there in the file1 but not in file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5ba0e93-ee33-4bd0-ba23-ec40214fac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8b38461-77ea-45f9-8a17-3a652661f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop is bigdata tool\n",
      "Sark is meant for only processing\n",
      "Hadoop is good for analytics\n",
      "Spark is purely depending on IN_MEMORY processing\n",
      "Hadoop and Spark are going to rule bigdata market\n"
     ]
    }
   ],
   "source": [
    "for word in result.collect(): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ea97410-1ee5-4ff9-a0d3-774bae9714d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([12,90,3,32,10,56,98,43,56,21,17,41,37,18,87,76,56,20,67,51,63,88,76,18,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4c40190-b07d-49a5-a361-29fdba63bf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6040c6a5-6316-4ea9-a800-c9c9354bd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata  = data.sortBy(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8de1d4d-4a5c-4305-b807-df3496fe016c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 10,\n",
       " 12,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 32,\n",
       " 37,\n",
       " 41,\n",
       " 43,\n",
       " 51,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 63,\n",
       " 67,\n",
       " 76,\n",
       " 76,\n",
       " 87,\n",
       " 88,\n",
       " 90,\n",
       " 98]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17350ec2-631f-41bd-8a19-d6bed99aa2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata = data.sortBy(lambda x: x, ascending=True) # One more way of sorting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "013d689a-003b-4058-9605-82bd153be0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 10,\n",
       " 12,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 32,\n",
       " 37,\n",
       " 41,\n",
       " 43,\n",
       " 51,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 63,\n",
       " 67,\n",
       " 76,\n",
       " 76,\n",
       " 87,\n",
       " 88,\n",
       " 90,\n",
       " 98]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "144c34f2-48ea-418d-9f70-be624b863fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata_desc = data.sortBy(lambda x: x, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "494d1ac0-51de-4381-8ce8-ca34e0860e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98,\n",
       " 90,\n",
       " 88,\n",
       " 87,\n",
       " 76,\n",
       " 76,\n",
       " 67,\n",
       " 63,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 51,\n",
       " 43,\n",
       " 41,\n",
       " 37,\n",
       " 32,\n",
       " 21,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 12,\n",
       " 10,\n",
       " 3]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortdata_desc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfce014d-43cc-4e7b-842d-96301fcbe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata = data.sortBy(lambda x: x, ascending=True) # One more way of sorting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17cb44bc-5b41-4dbf-b899-42a77fb82f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = sc.parallelize([(\"Spark\", 40),(\"Scala\", 60),(\"Hadoop\", 20),(\"C++\", 40),(\"C\", 71),(\"Java\", 30),(\"Python\", 40),(\"RubyOnRails\", 79),(\"VC++\", 65),\n",
    "    (\"Cobol\", 10),(\"JavaScript\", 90)]) #PairedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af16163a-eca4-4e9d-a106-b65ddc1e81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata = datas.sortBy(lambda x: x[1], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9571d930-ce0b-40be-99f3-3c5674efd585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cobol', 10),\n",
       " ('Hadoop', 20),\n",
       " ('Java', 30),\n",
       " ('Spark', 40),\n",
       " ('C++', 40),\n",
       " ('Python', 40),\n",
       " ('Scala', 60),\n",
       " ('VC++', 65),\n",
       " ('C', 71),\n",
       " ('RubyOnRails', 79),\n",
       " ('JavaScript', 90)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92acfc02-6057-403c-b74e-d9fe37f0c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdata = datas.sortBy(lambda x: x[0], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "999754ad-74d3-4643-96c4-f3d7279896d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 71),\n",
       " ('C++', 40),\n",
       " ('Cobol', 10),\n",
       " ('Hadoop', 20),\n",
       " ('Java', 30),\n",
       " ('JavaScript', 90),\n",
       " ('Python', 40),\n",
       " ('RubyOnRails', 79),\n",
       " ('Scala', 60),\n",
       " ('Spark', 40),\n",
       " ('VC++', 65)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d891dbd-39de-4d7d-9638-554b002a161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(\"Spark\", 40),(\"Scala\", 60),(\"Hadoop\", 20),(\"C++\", 40),(\"C\", 50),(\"Java\", 30),(\"Python\", 50),(\"RubyOnRails\", 40),(\"VC++\", 70),\n",
    "                       (\"Cobol\", 10),(\"Spark\", 70),(\"Scala\", 30),(\"Scala\", 90),(\"Spark\", 50),(\"Spark\", 50),(\"Scala\", 30),(\"Spark\", 80),(\"Java\", 50),(\"Python\", 30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7265f6f-fbcf-42b1-bddd-e8f6249c37b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spark', 40)\n",
      "('Scala', 60)\n",
      "('Hadoop', 20)\n",
      "('C++', 40)\n",
      "('C', 50)\n",
      "('Java', 30)\n",
      "('Python', 50)\n",
      "('RubyOnRails', 40)\n",
      "('VC++', 70)\n",
      "('Cobol', 10)\n",
      "('Spark', 70)\n",
      "('Scala', 30)\n",
      "('Scala', 90)\n",
      "('Spark', 50)\n",
      "('Spark', 50)\n",
      "('Scala', 30)\n",
      "('Spark', 80)\n",
      "('Java', 50)\n",
      "('Python', 30)\n"
     ]
    }
   ],
   "source": [
    "for i in data.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b40debd-d8ea-4e5c-992a-4a65e3a7928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpData = data.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31a44518-37d6-4b1b-b3b8-a4ec73560e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RubyOnRails,[40]\n",
      "VC++,[70]\n",
      "Scala,[60, 30, 90, 30]\n",
      "C++,[40]\n",
      "Hadoop,[20]\n",
      "Java,[30, 50]\n",
      "C,[50]\n",
      "Cobol,[10]\n",
      "Python,[50, 30]\n",
      "Spark,[40, 70, 50, 50, 80]\n"
     ]
    }
   ],
   "source": [
    "for keys,values in grpData.collect():\n",
    "    print(f\"{keys},{list(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "610fbe5b-aa2c-4612-885f-b4422ab51a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = data.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8c8a65e-85f8-4f35-bbd5-ab580ed55cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RubyOnRails,40\n",
      "VC++,70\n",
      "Scala,210\n",
      "C++,40\n",
      "Hadoop,20\n",
      "Java,80\n",
      "C,50\n",
      "Cobol,10\n",
      "Python,80\n",
      "Spark,290\n"
     ]
    }
   ],
   "source": [
    "for keys,values in reduced_data.collect():\n",
    "    print(f\"{keys},{values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "710a51fb-da4e-45e3-acbd-2b8f5950d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpData1 = data.groupByKey().map(lambda x: (x[0], sum(x[1])))  #summing up with only groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08896c1e-4b8c-4e8d-a91b-89dd12d7fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RubyOnRails,40\n",
      "VC++,70\n",
      "Scala,210\n",
      "C++,40\n",
      "Hadoop,20\n",
      "Java,80\n",
      "C,50\n",
      "Cobol,10\n",
      "Python,80\n",
      "Spark,290\n"
     ]
    }
   ],
   "source": [
    "for key,values in grpData1.collect():\n",
    "    print(f\"{key},{values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "979c379b-d807-4aad-9f02-7e18ba622051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db817b8-8764-4165-acfe-82f5c55e4f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3.11",
   "language": "python",
   "name": "pyspark3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
